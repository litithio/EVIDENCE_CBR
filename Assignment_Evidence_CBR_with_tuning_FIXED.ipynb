{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Optimizer (Fixed Copy)\n",
    "\n",
    "Diese Notebook‑Kopie ist JSON‑valide und enthält nur die robusten Abschluss‑Zellen:\n",
    "- Optimierer mit `OPTIMIZE`‑Schalter und feinem tau/Ω‑Raster.\n",
    "- Finale Auswertung (Konfusionsmatrix, P/R/F1, Vergleich).\n",
    "- Visual Vergleiche (Balken) und Streuung richtig/falsch.\n",
    "\n",
    "Voraussetzungen (im selben Kernel ausführen wie das Tuning‑Notebook):\n",
    "- `data`: DataFrame mit Spalte `Bike Type` und Soft‑Voting Ergebnissen (bevorzugt `Pred_DS_soft_tuned`).\n",
    "- `masses`: Liste der Baseline‑MassFunctions (eine pro Zeile in `data`).\n",
    "- `omega`: Schlüsselname für Ω in den MassFunctions (z. B. `'omega'`).\n",
    "\n",
    "Ablauf:\n",
    "1) Optimierer‑Zelle ausführen (setzt `data['Pred_DS_final']`).\n",
    "2) Finale Auswertung, dann Visual‑Zellen ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "# Zielmetrik: 'accuracy' oder 'macro_f1'\n",
    "OPTIMIZE = 'macro_f1'  # 'accuracy' | 'macro_f1'\n",
    "tau_list = np.arange(0.46, 0.53, 0.01)\n",
    "omega_list = np.arange(0.46, 0.53, 0.01)\n",
    "label_map = {'r':'race bike','m':'mtb','t':'trecking bike'}\n",
    "classes = ['r','m','t']\n",
    "\n",
    "# Voraussetzungen prüfen\n",
    "missing = []\n",
    "for name in ['data','masses','omega']:\n",
    "    if name not in globals(): missing.append(name)\n",
    "if missing:\n",
    "    raise RuntimeError(f'Fehlende Objekte im Kernel: {missing}. Bitte erst das Tuning‑Notebook bis inkl. Hybrid ausführen und diesen Kernel übernehmen.')\n",
    "\n",
    "fb = None\n",
    "for c in ['Pred_DS_soft_tuned','Pred_DS_soft_best','Pred_DS_soft']:\n",
    "    if c in data.columns: fb = c; break\n",
    "if fb is None:\n",
    "    raise RuntimeError('Bitte Soft‑Voting zuerst ausführen (Pred_DS_soft_tuned oder Pred_DS_soft).')\n",
    "\n",
    "def evaluate_hybrid(tau_singleton, omega_max):\n",
    "    preds = []\n",
    "    for i, row in data.iterrows():\n",
    "        m = masses[i] if i < len(masses) else None\n",
    "        if m is None:\n",
    "            preds.append(row.get(fb, None)); continue\n",
    "        singles = {c: (m[c] if c in m else 0.0) for c in classes}\n",
    "        best_c, best_v = max(singles.items(), key=lambda kv: kv[1])\n",
    "        om_v = float(m[omega]) if omega in m else 0.0\n",
    "        preds.append(label_map[best_c] if (best_v >= tau_singleton and om_v <= omega_max) else row.get(fb, None))\n",
    "    df_eval = data[data['Bike Type'].notna() & (data['Bike Type']!='')].copy()\n",
    "    y_true = df_eval['Bike Type']\n",
    "    y_pred = pd.Series(preds, index=data.index).loc[df_eval.index]\n",
    "    cm = pd.crosstab(y_true, y_pred).reindex(index=['race bike','mtb','trecking bike'], columns=['race bike','mtb','trecking bike'], fill_value=0)\n",
    "    diag = np.diag(cm.values); support = cm.sum(axis=1).values; pred_sum = cm.sum(axis=0).values\n",
    "    rec = np.divide(diag, support, out=np.zeros_like(diag, float), where=support>0)\n",
    "    prec = np.divide(diag, pred_sum, out=np.zeros_like(diag, float), where=pred_sum>0)\n",
    "    f1 = np.divide(2*prec*rec, prec+rec, out=np.zeros_like(diag, float), where=(prec+rec)>0)\n",
    "    macro_f1 = float(np.nanmean(f1)); acc = float((y_pred==y_true).mean())\n",
    "    return acc, macro_f1, preds\n",
    "\n",
    "# Raster durchsuchen\n",
    "records = []; best = (-1.0, -1.0, None, None); best_preds=None\n",
    "for t in tau_list:\n",
    "    for o in omega_list:\n",
    "        acc, mf1, preds = evaluate_hybrid(t,o)\n",
    "        records.append({'tau':t,'omega':o,'acc':acc,'macro_f1':mf1})\n",
    "        better = (mf1>best[1] or (mf1==best[1] and acc>best[0])) if OPTIMIZE=='macro_f1' else (acc>best[0] or (acc==best[0] and mf1>best[1]))\n",
    "        if better: best=(acc,mf1,t,o); best_preds=preds\n",
    "\n",
    "# Ergebnis ausgeben und speichern\n",
    "df_res = pd.DataFrame.from_records(records)\n",
    "acc, mf1, t, o = best\n",
    "print(f\"Best Optimizer -> acc={acc:.3f} | macroF1={mf1:.3f} | tau={t} | Ω_max={o} | OPT={OPTIMIZE}\")\n",
    "data['Pred_DS_final'] = best_preds\n",
    "\n",
    "# Heatmap der gewählten Metrik\n",
    "metric = 'macro_f1' if OPTIMIZE=='macro_f1' else 'acc'\n",
    "pivot = df_res.pivot(index='tau', columns='omega', values=metric)\n",
    "plt.figure(figsize=(6,4)); sns.heatmap(pivot.sort_index(), annot=True, fmt='.3f', cmap='viridis'); plt.title(f'Hybrid Grid {OPTIMIZE}'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale Auswertung (Konfusionsmatrix + P/R/F1 + Baseline/Final‑Vergleich)\n",
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "if 'Pred_DS_final' not in data.columns:\n",
    "    raise RuntimeError('Bitte zuerst den Optimierer ausführen (Pred_DS_final fehlt).')\n",
    "\n",
    "df_eval = data[data['Bike Type'].notna() & (data['Bike Type']!='')].copy()\n",
    "cm_f = pd.crosstab(df_eval['Bike Type'], df_eval['Pred_DS_final']).reindex(index=['race bike','mtb','trecking bike'], columns=['race bike','mtb','trecking bike'], fill_value=0)\n",
    "diag_f = np.diag(cm_f.values); support_f = cm_f.sum(axis=1).values; pred_sum_f = cm_f.sum(axis=0).values\n",
    "rec_f = np.divide(diag_f, support_f, out=np.zeros_like(diag_f, float), where=support_f>0)\n",
    "prec_f = np.divide(diag_f, pred_sum_f, out=np.zeros_like(diag_f, float), where=pred_sum_f>0)\n",
    "f1_f = np.divide(2*prec_f*rec_f, prec_f+rec_f, out=np.zeros_like(diag_f, float), where=(prec_f+rec_f)>0)\n",
    "macro_f1_f = float(np.nanmean(f1_f))\n",
    "acc_f = float((df_eval['Pred_DS_final'] == df_eval['Bike Type']).mean())\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_f.div(cm_f.sum(axis=1).replace(0,np.nan), axis=0).fillna(0), annot=True, fmt='.2f', cmap='Greens')\n",
    "plt.title('Konfusionsmatrix Final (zeilennormiert)'); plt.xlabel('Vorhersage (final)'); plt.ylabel('Wahr'); plt.tight_layout(); plt.show()\n",
    "print('Per‑Klasse (Final) P/R/F1:')\n",
    "for cls, p,r,f,s in zip(cm_f.index, prec_f, rec_f, f1_f, support_f):\n",
    "    print(f'{cls:14s}  P={p:.3f}  R={r:.3f}  F1={f:.3f}  (n={int(s)})')\n",
    "\n",
    "# Baseline Vergleich (falls baseline_metrics nicht existiert, wird on‑the‑fly berechnet)\n",
    "if 'baseline_metrics' in globals():\n",
    "    acc_b = float(baseline_metrics['accuracy'])\n",
    "    f1_b = float(np.mean([v['f1'] for v in baseline_metrics['per_class'].values()]))\n",
    "else:\n",
    "    cm_b = pd.crosstab(df_eval['Bike Type'], df_eval['Pred_DS']).reindex(index=['race bike','mtb','trecking bike'], columns=['race bike','mtb','trecking bike'], fill_value=0)\n",
    "    diag_b = np.diag(cm_b.values); support_b = cm_b.sum(axis=1).values; pred_sum_b = cm_b.sum(axis=0).values\n",
    "    rec_b = np.divide(diag_b, support_b, out=np.zeros_like(diag_b, float), where=support_b>0)\n",
    "    prec_b = np.divide(diag_b, pred_sum_b, out=np.zeros_like(diag_b, float), where=pred_sum_b>0)\n",
    "    f1_bv = np.divide(2*prec_b*rec_b, prec_b+rec_b, out=np.zeros_like(diag_b, float), where=(prec_b+rec_b)>0)\n",
    "    f1_b = float(np.nanmean(f1_bv))\n",
    "    acc_b = float((df_eval['Pred_DS'] == df_eval['Bike Type']).mean())\n",
    "summary = pd.DataFrame({'Accuracy':[acc_b, acc_f],'Macro_F1':[f1_b, macro_f1_f]}, index=['Baseline','Final'])\n",
    "display(summary.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: Baseline vs Final Balken‑Vergleich je Klasse (Precision/Recall/F1)\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "df_eval = data[data['Bike Type'].notna() & (data['Bike Type']!='')].copy()\n",
    "classes = ['race bike','mtb','trecking bike']\n",
    "# Baseline\n",
    "if 'baseline_metrics' in globals():\n",
    "    base_prec = [baseline_metrics['per_class'][c]['precision'] for c in classes]\n",
    "    base_rec  = [baseline_metrics['per_class'][c]['recall']    for c in classes]\n",
    "    base_f1   = [baseline_metrics['per_class'][c]['f1']       for c in classes]\n",
    "else:\n",
    "    cm_b = pd.crosstab(df_eval['Bike Type'], df_eval['Pred_DS']).reindex(index=classes, columns=classes, fill_value=0)\n",
    "    diag_b = np.diag(cm_b.values); support_b = cm_b.sum(axis=1).values; pred_sum_b = cm_b.sum(axis=0).values\n",
    "    base_rec = np.divide(diag_b, support_b, out=np.zeros_like(diag_b, float), where=support_b>0)\n",
    "    base_prec = np.divide(diag_b, pred_sum_b, out=np.zeros_like(diag_b, float), where=pred_sum_b>0)\n",
    "    base_f1 = np.divide(2*base_prec*base_rec, base_prec+base_rec, out=np.zeros_like(diag_b, float), where=(base_prec+base_rec)>0)\n",
    "# Final\n",
    "cm_f = pd.crosstab(df_eval['Bike Type'], df_eval['Pred_DS_final']).reindex(index=classes, columns=classes, fill_value=0)\n",
    "diag_f = np.diag(cm_f.values); support_f = cm_f.sum(axis=1).values; pred_sum_f = cm_f.sum(axis=0).values\n",
    "fin_rec = np.divide(diag_f, support_f, out=np.zeros_like(diag_f, float), where=support_f>0)\n",
    "fin_prec = np.divide(diag_f, pred_sum_f, out=np.zeros_like(diag_f, float), where=pred_sum_f>0)\n",
    "fin_f1 = np.divide(2*fin_prec*fin_rec, fin_prec+fin_rec, out=np.zeros_like(diag_f, float), where=(fin_prec+fin_rec)>0)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4), sharey=False)\n",
    "metrics = [('Precision', base_prec, fin_prec), ('Recall', base_rec, fin_rec), ('F1', base_f1, fin_f1)]\n",
    "x = np.arange(len(classes)); width = 0.38\n",
    "for ax, (title, base_vals, fin_vals) in zip(axes, metrics):\n",
    "    ax.bar(x - width/2, base_vals, width, label='Baseline')\n",
    "    ax.bar(x + width/2, fin_vals,  width, label='Final')\n",
    "    ax.set_title(title); ax.set_xticks(x); ax.set_xticklabels(classes, rotation=15)\n",
    "    ax.set_ylim(0,1); ax.grid(True, axis='y', alpha=0.3)\n",
    "axes[0].set_ylabel('Wert'); axes[-1].legend(loc='lower right'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: Scatter Distance vs Elevation (grün = korrekt, rot = falsch; Marker = wahre Klasse)\n",
    "import matplotlib.pyplot as plt\n",
    "df_eval = data[data['Bike Type'].notna() & (data['Bike Type']!='')].copy()\n",
    "df_eval['CorrectFinal'] = (df_eval['Pred_DS_final'] == df_eval['Bike Type'])\n",
    "markers = {'race bike':'o','mtb':'s','trecking bike':'^'}\n",
    "plt.figure(figsize=(7,5))\n",
    "for truth, sub in df_eval.groupby('Bike Type'):\n",
    "    plt.scatter(sub['Distance'], sub['Elevation Gain'], c=sub['CorrectFinal'].map({True:'tab:green',False:'tab:red'}),\n",
    "                marker=markers.get(truth,'o'), alpha=0.8, edgecolor='k', linewidths=0.2, label=truth)\n",
    "plt.xlabel('Distance'); plt.ylabel('Elevation Gain'); plt.title('Final: richtig (grün) vs. falsch (rot)');\n",
    "plt.grid(True, alpha=0.3); plt.legend(title='Wahre Klasse'); plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
